<!DOCTYPE html> 
<head>
    <title>Задание #7</title>
	<meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="stylet.css">
<body>
<p><b>Задание #8. Улучшенный поисковый робот</b></p>
<p>Сделанный вами в прошлом задании поисковый робот не очень эффективен. На этой неделе вы расширите его возможности применив потоки Java. Робот сможет обрабатывать несколько страниц параллельно. Это должно существенно улучшить производительность программы потому, что всякий раз когда какой либо поток робота будет ждать завершения загрузки страницы по сети, другие потоки смогут продолжить свои операции по обработке данных на своих страницах.</p>
<p>Подробное введение в многозадачное программирование на языке Java ( <em>очень</em> сложный вопрос!), вы можете прочитать в <a href="http://download.oracle.com/javase/tutorial/essential/concurrency/index.html" target="_blank">этом учебном руководстве</a>. Наиболее важный раздел <a href="http://download.oracle.com/javase/tutorial/essential/concurrency/sync.html" target="_blank">этот</a>.</p>
<p><b>Улучшение поискового робота</b></p>
<p>Вам нужно добавить новые возможности к коду написанному в прошлом задании:</p>
<ol>
<li>
<p>Сделайте класс с именем <tt>URLPool</tt>, который сохраняет список всех найденных URLs, вместе с "уровнем" каждого из этих URL (мы называем это также "глубина поиска"). Первый URL с которого вы начали поиск имеет глубину поиска 0. Все URL найденные на этой странице имеют глубину 1, и так далее. Сохраняйте URL и их глубину вместе как экземпляр класса <tt>URLDepthPair</tt> также как вы это делали в прошлом задании. Для хранения элементов рекомендуется использовать <tt>LinkedList</tt>, так как он очень эффективно выполняет требуемые операции.</p>
<p>Нужно чтобы пользователь класса <tt>URLPool</tt> мог извлекать пару URL-глубина из пула, и одновременно удалять ее из списка. Так же нужно обеспечить возможность добавления пары URL-глубина к пулу. Обе эти операции должны быть рассчитаны на вызов из нескольких потоков так как несколько потоков могут одновременно работать с <tt>URLPool</tt>.</p>
<p><b>В отличие от примера очереди FIFO, который рассматривался в лекциях, пул URL не должен иметь ограничений размера.</b> В него нудно добавить список URL ожидающих обработки, список просмотренных URL и еще одно поле, которое мы рассмотрим ниже.</p>
</li>
<li>
<p>Для того чтобы поиск можно было выполнять в нескольких потоках, создайте класс <tt>CrawlerTask</tt>, который реализует интерфейс <tt>Runnable</tt>. Каждый экземпляр <tt>CrawlerTask</tt> должен иметь ссылку на <em>единственный</em> экземпляр класса <code>URLPool</code> который описан выше. (Обратите внимание, что <em>все</em> экземпляры <tt>CrawlerTask</tt> используют один общий пул ссылок!) Поток поискового робота должен работать так:</p>
<ol>
<li>Извлекает пару URL-глубина из пула (ждет если в данный момент ссылок нет).</li>
<li>Загружает веб страницу на которую ссылается URL.</li>
<li>Ищет на странице другие URL. Для каждого найденного на странице поток робота должен добавить новую пару URL-глубина в пул ссылок. Новая пара должна иметь глубину на единицу большую, чем URL из которого эта ссылка извлечена на шаге 1.</li>
<li>Возвращается к шагу 1!</li>
</ol>
<p>Эти операции следует продолжать до тех пор, пока в пуле ссылок не останется ни одной пары (URL, глубина).</p>
</li>
<li>
<p>Так как ваш робот запускает некоторое количество потоков, добавьте в программу третий параметр командной строки, задающий число потоков робота. Ваша функция main должна работать примерно так:</p>
<ol>
<li>Обрабатывает аргументы командной строки. Сообщает пользователю об ошибках, если они есть.</li>
<li>Создает экземпляр класса пула URL, и помещает указанный пользователем URL с глубиной 0 в пул.</li>
<li>Создает указанное пользователем число задач робота (и потоков для их запуска). Каждая задача робота должна иметь ссылку на пул URL, который только что создан.</li>
<li>Ожидает пока робот не закончит работу! (Об этом ниже.)</li>
<li>Печатает в результате список найденных URL.</li>
</ol></li>
<li>
<p><i>Синхронизируйте</i> доступ к объекту пула URL во всех важных местах, потому что доступ к этому объекту из потоков должен быть безопасным.</p>
</li>
<li>
<p>Робот извлекая очередную ссылку не должен непрерывно опрашивать пул URL если он пуст. Он должен жать появления URL более эффективно. Метод пула URL "получить URL" должен сам <tt>ждать используя функцию wait()</tt>, если в пуле в момент вызова нет ссылок. Соответственно в метод пула ссылок "добавить URL" надо добавить вызов <tt>notify()</tt>, когда новая ссылка добавлена в пул.</p>
<p><em>Заметим, что потоки робота сами по себе не выполняют операции synchronize / wait / notify.</em> Это следует сделать по той же причине, по которой пул URL прячет внутри себя детали реализации добавления и извлечения ссылок: <b>инкапсуляция!</b> Так же как вы хотите избавить пользователей от деталей реализации пула URL, вы избавляете их от деталей синхронизации потоков.</p>
</li>
</ol>
<p><b>Советы по проектированию</b></p>
<p>Вот некоторые полезные советы по выполнению задания 8!</p>
<ul>
<li>
<p>Возможно вы перенесете с небольшими изменениями значительные фрагменты кода из прошлого задания в новое. Класс <tt>URLDepthPair</tt> вероятно вовсе не нужно изменять. Большая часть кода обрабатывающего страницу также может быть перенесена без изменений. Основное отличие от предыдущего задания в том, что код загрузки и обработки URL теперь должны быть помещен в класс реализующий <tt>Runnable</tt>, и теперь код должен извлекать и добавлять ссылки в экземпляр класса <tt>URLPool</tt>.</p>
</li>
<li>
<p>Надо синхронизировать доступ к внутренним полям <tt>URLPool</tt>, так как к ним будут обращаться из нескольких потоков. Как обсуждалось на лекции, наиболее простой подход, это использование синхронизированных методов (<tt>synchronized</tt>) вместо вставки блоков <tt>synchronized</tt> в методах класса. Не нужно синхронизировать конструктор <tt>URLPool</tt>! Подумайте о том, какие методы нужно синхронизировать.</p>
</li>
<li>
<p>Напишите методы <tt>URLPool</tt>, которые используют <tt>wait()</tt> и <tt>notify()</tt> так, чтобы робот мог эффективно ждать пока ссылки появятся в пуле.</p>
</li>
<li>
<p>Пусть <tt>URLPool</tt> контролирует добавление ссылок в список на обработку, проверяя глубину каждой добавляемой в пул ссылки. Если глубина URL меньше максимальной, пара URL-глубина добавляется в список ссылок для обработки. Если нет, просто добавьте URL в список "просмотренных" ссылок.</p>
</li>
<li>
<p>Наименее понятная часть работы, это определение условия завершения исполнения программы, в случае, если не осталось ссылок для просмотра (или достигнута максимальная глубина просмотра). Когда это происходит, все потоки <tt>CrawlerTask</tt> переходят в состояние ожидания (<tt>wait()</tt>) появления новой ссылки в <tt>URLPool</tt>. Рекомендуется в <tt>URLPool</tt> следить за количеством потоков ожидающих новую ссылку. Для этого следует добавить поле типа <tt>int</tt>, которое инкрементируется перед вызовом <tt>wait()</tt>, и декрементируется сразу после того как <tt>wait()</tt> возвращает значение.</p>
<p>Когда у вас есть счетчик числа ждущих потоков, можно добавить (синхронизированный!) метод возвращающий число таких потоков. Когда этот счетчик станет равен общему числу потоков робота, время завершать работу программы. Этот счетчик можно проверять в функции <tt>main()</tt>, a вызывать <tt>System.exit()</tt> для завершения работы. Это конечно еще один пример ужасного поллинга, но его стоимость можно свести на нет, переводя поток main в спящее состояние на короткое время между этими проверками. Значение между 0.1 секунды и 1 секундой вероятно наиболее подходящее значения периода опроса.</p>
</li>
</ul>
<p><b>Дополнительные баллы</b></p>
<ul>
<li>
<p>Сделайте так, чтобы в паре URL-глубина использовался класс <tt>java.net.URL</tt>, и добавьте роботу возможность обрабатывать <em>относительные</em> URL так же как и абсолютные. Для этого нужно немного изменить код, но это не слишком сложно.</p>
</li>
<li>
<p>Завершение программы с помощью <tt>System.exit()</tt> не лучший способ сделать это. Найдите лучший способ выхода из программы после того, как поиск закончен.</p>
</li>
<li>
<p>Сделайте общий список обработанных ссылок, и сделайте так, чтобы не допускалась повторная обработка ссылок. Используйте один из видов коллекций Java, который упрощает эту задачу. Лучше всего использовать некоторые виды множеств, которые имеют постоянное время поиска и вставки.</p>
</li>
<li>
<p>Добавьте параметр командной строки, который задает время ожидания ответа от сервера во время загрузки веб страницы. Используйте этот параметр как порог для <i>начального</i> ожидания загрузки страницы, а затем добавьте другой параметр <code>maxPatience</code> который задает время обработки страницы, после которого обработка прерывается и происходит переход к следующей странице.</p>
</li>
<li>
<p>Итак, cделайте ваш конкурент системе поиска Google ;-)</p>
</li>
</ul>
<div align="center"><hr size="2" width="100%" noshade="noshade" align="center" /></div>
<p>Copyright (C) 2015, California Institute of Technology. All rights reserved.</p>
</body>
</html>