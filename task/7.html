<!DOCTYPE html> 
<head>
    <title>Задание #7</title>
	<meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="stylet.css">
<body>
<p><b>Задание #7. Поисковый робот</b></p>
<p>В этой лабораторной работе, требуется написать простейший <b>поисковый робот</b>. Робот должен автоматически загружать веб страницы из сети Интернет, искать в них новые ссылки, и повторять эту операцию для каждой найденной ссылки. В этой лабораторной работе поисковый робот будет настолько простым, насколько можно себе это представить. Он будет просто просматривать новые URL (указывающее на расположение других веб страниц) на каждой странице, сохранять эти ссылки и печатать их в конце работы программы. Более сложные поисковые роботы используются, например,  для индексирования содержимого сети Интернет или  сбора адресов электронной почты для рассылки спама. Если вы когда либо пользовались <a href="http://www.google.com/">поисковым сервисом</a>, то вы искали в данных созданных поисковым  роботом.</p>
<p><b>Терминология</b></p>
<p><b>URL</b>: Uniform Resource Locator. Единый указатель ресурсов. Адрес веб страницы. В нашем случае, он содержит строку "http://", за которой следует имя веб сервера (например,<tt>"www.cs.caltech.edu"</tt>), за которым следует путь к веб странице на сервере  (например, <tt>"/courses/cs11"</tt>). Если этот путь не заканчивается расширением ".html" сервер решает сам какую страницу он должен возвратить. Это может быть <tt>index.html</tt>, <tt>index.shtml</tt>, <tt>index.php</tt>, <tt>default.htm</tt>, или что то еще, что сервер считает документом “по умолчанию” для указанного каталога.</p>
<p><i>Замечание:</i> Есть и другие типы URL, начинающиеся (например) с <tt>"mailto://"</tt> или <tt>"ftp://"</tt>. Эти типы URL нас здесь не интересуют.</p>
<p><b>HTTP</b>: HyperText Transfer Protocol. Протокол передачи гипертекста. Это текстовый протокол, используемый для передачи веб страниц через интернет. Последняя спецификация протокола  HTTP имеет версию 1.1, которую мы и будем использовать. Запрос HTTP для загрузки страницы</p>
<pre>  http://www.cs.caltech.edu/courses/cs11</pre>
<p>может выглядеть примерно так:</p>
<pre>  GET /courses/cs11 HTTP/1.1
  Host: www.cs.caltech.edu
  Connection: close
  [дополнительная пустая строка]</pre>
<p>Заметим, что запрос HTTP ДОЛЖЕН завешаться пустой строкой, иначе запрос не будет обработан сервером. Так же обратите внимание на использование заглавных букв. Если вы отправите в запросе Get или host сервер также не будет это обрабатывать.</p>
<p>В некоторых URL не указывается документ или ресурс, который нужно извлечь, например: "<tt>http://www.caltech.edu</tt>". В таких случаях, <b>необходимо</b> указать вместо ресурса обратную косую черту "<tt>/</tt>". Другими словами, путь к ресурсу <i>всегда</i> должен начинаться с символа <tt>/</tt>.</p>
<p><b>Socket</b>: Сокет, это ресурс операционной системы использующийся для установки соединения с другим компьютером в сети. Сокет можно использовать для установки соединения с веб сервером, для этого нужно использовать TCP сокет, и передавать через него пакеты протокола HTTP.</p>
<p><b>Порт</b>: Различные программы установленные на одном сервере могут ожидать соединений , прослушивая различные порты. Каждый порт имеет номер в диапазоне 1..65535;  Номера от 1 до 1024 зарезервированы операционной системой. Большинство сервисов имеют порты по умолчанию; Для соединений HTTP обычно используется порт 80.</p>
<p><b>Программа, которую нужно написать</b><p>
<p>Ниже приведена спецификация программы, которую вы должны написать.</p>
<p>Программа должна иметь два аргумента в командной строке:</p>
<ol start="1">
<li>Строка, содержащая URL страницы с которой начинается поиск</li>
<li>Положительное целое число – максимальная глубина поиска (см. ниже)</li>
</ol>
<p>Если программа получает некорректные аргументы, она должна немедленно прекратить работу и напечатать сообщение, содержащее правила вызова, например:</p>
<p> Правила вызова: java Crawler &lt;URL&gt; &lt;глубина поиска&gt;</p>
<p>(Дополнительную информацию об обработке аргументов командной строки можно посмотреть в <a href="http://courses.cms.caltech.edu/cs11/material/java/donnie/java-main.html">этом документе</a>.)</p>
<p>Программа должна сохранять URL в виде строки<tt>String</tt> вместе с глубиной (которая, в начале, равна 0). Для этого вы должны создать класс, содержащий пару значений [URL, глубина].</p>
<p>Программа должна подключаться к 80 порту заданного в URL сайта, используя класс <tt>Socket</tt>(см. ниже) и запрашивать указанную веб страницу,</p>
<p>Программа должна разбирать полученный в ответ текст (если он получен), строка за строкой отыскивая фрагменты строк такого формата:</p>
<pre>  &lt;a href="[какой либо<i> </i><i>URL начинающийся с</i> http://]"&gt;</pre>
<p>Найденные URL  должны сохраняться, вместе с новым значением глубины ссылки в списке  <tt>LinkedList</tt> объектов (URL, глубина)  (см. ниже подробности про <tt>LinkedList</tt>s). Новое значение глубины должно быть на единицу больше глубины URL обрабатываемой страницы.</p>
<p>Затем программа должна закрывать соединение с хостом.</p>
<p>Программа должна повторять шаги с  3 по 6 для каждого нового URL, до тех пор, пока глубина URL меньше максимальной глубины просмотра. Заметим, что при извлечении и поиске внутри  URL глубина увеличивается на 1. Если глубина URL достигает максимума, не загружайте и не ищите новые ссылки в веб странице.</p>
<p>Перед завершением программа должна выводить все найденные URL в пределах заданной глубины поиска.</p>
<p><b>Допущения</b></p>
<p>Довольно трудно разобрать и еще труднее подключиться ко всем без исключения ссылкам, имеющим правильный или ошибочный формат. Предположим, что каждая ссылка имеет правильный формат, включающий полное имя хоста, путь к ресурсу, и заключена в двойные кавычки. Как это не удивительно,  есть несколько больших веб сайтов, в который имеется много ссылок такого вида; можете попробовать этот <tt>http://slashdot.org/, или этот  <tt>http://www.nytimes.com</tt>. (Кстати, обратная косая черта после slashdot.org, важна ...)</tt></p>
<p>Заметим, что большинство ссылок которые мы не обрабатываем это те которые начинаются не с "http://"; это могут быть, например, ссылки  "mailto://" или "ftp://". Не обрабатывайте такие ссылки.</p>
<p>Допустим, что если <tt>BufferedReader</tt> возвращает <tt>null</tt>, сервер завершил отправку страницы. Это в действительности может быть не так, в случае, если сервер работает очень медленно, но для наших целей это вполне приемлемо.</p>
<h3>Полезные классы и методы</h3>
<p>Как всегда детали выясняйте в документации Java API. Классы и методы, о которых будет сказано ниже, помогут вам начать работу. Большинство этих методов вызывает различные виды исключений, которые вы должны обрабатывать. Подробности также посмотрите в документации Java API.</p>
<p><b>Socket</b></p>
<p>Для того чтобы использовать класс <tt>Socket</tt> к программе надо добавить строку:</p>
<pre>  import java.net.*;</pre>
<p><b>Конструктор</b></p>
<p><tt>Socket(String host, int port)</tt> создает новый объект <tt>Socket</tt> из строки <tt>String</tt> содержащей имя хоста и номер порта, и устанавливает соединение.</p>
<p><b>Методы</b></p>
<p><tt>void setSoTimeout( int timeout)</tt> устанавливает таймаут сокета в миллисекундах. Метод надо вызвать после создания объекта <tt>Socket</tt> чтобы задать время ожидания передачи данных с другой стороны соединения. Иначе вы будете ждать бесконечно долго, и это вероятно плохая идея для поискового робота. <tt>InputStream getInputStream()</tt> возвращает <tt>InputStream</tt> связанный с объектом <tt>Socket</tt> используемый для приема данных. OutputStream getOutputStream() возвращает <tt>OutputStream</tt> связанный с сокетом используемый для передачи данных. <tt>void close()</tt> закрывает <tt>Socket</tt>.</p>
<p><b>Потоки</b></p>
<p>Для того чтобы использовать потоки в программу надо добавить строку:</p>
<pre>  import java.io.*;</pre>
<p>Для эффективного использования сокетов, объекты <tt>InputStream</tt> и <tt>OutputStream</tt> надо конвертировать во что то более удобное для работы. Экземпляры классов  <tt>InputStream</tt> и <tt>OutputStream</tt> это очень простые объекты; они могут только считывать байты или массивы байтов (даже не символы). Так как нам требуется читать и записывать символы, нужны объекты способные конвертировать байты в символы и наоборот. К сожалению Java API делает это разными способами для входных и выходных потоков.</p>
<p><b>Входные потоки</b></p>
<p>Для входных потоков можно использовать класс <tt>InputStreamReader</tt>:</p>
<p>  InputStreamReader in = new InputStreamReader(my_socket.getInputStream());</p>
<p>Теперь <tt>in</tt> это объект <tt>InputStreamReader</tt> который может считывать символы из объекта <tt>Socket</tt>. Однако, это и сейчас не очень удобно потому что приходится работать с отдельными символами  (<tt>char</tt>) или массивами символов. Было бы хорошо считывать целиком строки. Для этого используется класс <tt>BufferedReader</tt>. Объект <tt>BufferedReader</tt> можно создать из экземпляра класса <tt>InputStreamReader</tt> и затем вызывать метод <tt>readLine</tt> объекта <tt>BufferedReader</tt>. Метод считывает целую строку из другой стороны соединения сокета.</p>
<p><b>Выходные потоки</b></p>
<p>Работа с выходными потоками организована проще. Создаем экземпляр класса <tt>PrintWriter</tt> прямо из объекта сокета <tt>OutputStream</tt> и затем вызываем его метод <tt>println</tt> для передачи строки текста на другую сторону соединения сокета. Надо использовать такой конструктор:</p>
<p>PrintWriter(OutputStream out, boolean autoFlush)</p>
<p>с <tt>autoFlush</tt> = <tt>true</tt>. Тогда данные не будут попадать в буфер передатчика, но будут передаваться сразу после каждого вызова <tt>println</tt>.</p>
<p><b>Методы класса <tt>String</tt></b></p>
<p>Вам могут пригодиться  нижеследующие методы класса <tt>String</tt>. См. также документацию API.</p>
<pre>boolean equals(Object anObject)
String substring(int beginIndex)
String substring(int beginIndex, int endIndex)
boolean startsWith(String prefix)</pre>
<p><b>ЗАМЕЧАНИЕ:</b> Не используйте оператор <tt>==</tt> для проверки равенства объектов <tt>String</tt>! Оператор возвращает <tt>true</tt> только, если обе переменные <tt>Strings</tt> ссылки на один и тот же объект. Для сравнения содержимого двух строк, используйте метод <tt>equals</tt>.</p>
<p><b>Cписки <tt>List</tt></b></p>
<p>Списки <tt>(List)</tt> очень похожи на массивы объектов <tt>(Object)</tt> но их можно расширять или уменьшать, если это необходимо, и они не используют нотацию с квадратными скобками для доступа к отдельным элементам. Для использования  <tt>List</tt> в программу надо добавить строку:</p>
<pre>  import java.util.*;</pre>
<p>Пары значений (URL, глубина) можно хранить в <tt>LinkedList</tt>. Это одна из реализаций класса <tt>List</tt>. Создается так:</p>
<pre>LinkedList&lt;URLDepthPair&gt; myList = new LinkedList&lt;URLDepthPair&gt;();</pre>
<p>Посмотрите сами документацию на большой набор методов <tt>List</tt>s и различных вариантов реализации <tt>Lists</tt>. (Обратите внимание на то, разные реализации <tt>List</tt> имеют разные свойства. Рекомендуем использовать <tt>LinkedList</tt>; некоторые свойства этого класса очень удобно использовать для решения нашей задачи.)</p>
<p>Особенный синтаксис, использованный выше для инициализации <tt>LinkedList</tt>, использует принципы <a href="http://java.sun.com/j2se/1.5.0/docs/guide/language/generics.html">обобщенного программирования  Java 1.5</a> . Использование такого синтаксиса позволяет убрать явное преобразование типов элементов при удалении и добавлении их к списку, и избавляет вас от связанной с этим головной боли.</p>
<p><b>Исключения</b></p>
<p>Если вы встречаете нечто выглядящее как URL, но не начинается с "<tt>http://</tt>", вы должны вызвать исключение типа <tt>MalformedURLException</tt>, которое имеется в Java API.</p>
<p><b>Рекомендации по проектированию</b></p>
<p>Вот некоторые рекомендации по реализации вашего поискового робота. (Не следование этим рекомендациям может привести к необходимости переделки работы ...<em>!</em>)</p>
<p><b>Пара URL-Глубина просмотра</b></p>
<p>Как уже было сказано, вы должны создать класс <tt>URLDepthPair</tt>, каждый экземпляр которого в поле типа <tt>String</tt> будет хранить URL и в поле типа <tt>int</tt> текущую глубину поиска. К классу надо добавить метод <tt>toString</tt><tt>,</tt>  который печатает пару значений на экране. Это облегчит вам вывод результатов работы поискового робота.</p>
<p>URL возможно потребуется разбивать на фрагменты. Разбор этих URL следует реализовать в созданном вами классе для хранения пар URL-глубина. Хороший объектно-ориентированный подход к программированию подразумевает, что если класс хранит какие либо данные, реализация процедур обработки этих данных должна быть реализована в этом же классе. Следовательно, если вам нужны функции, разделяющие URL на фрагменты, или функции проверки правильности формата URL добавьте их к этому классу!</p>
<p><b>Класс Crawler</b></p>
<p>Как уже было сказано, вы должны спроектировать класс <tt>Crawler</tt><tt>,</tt>  который реализует основной функционал приложения. Этот класс должен иметь метод <tt>getSites</tt> который возвращает весь список пар URL-глубина которые были просмотрены роботом. Его можно вызвать из метода <tt>main</tt> после завершения поиска; получить список, затем перебрать его элементы и вывести на экран все имеющиеся в нем ссылки.</p>
<p>Самый простой способ контролировать просматриваемые ссылки, это завести два списка. Один должен содержать все известные на данный момент ссылки,  а другой, включать еще не просмотренные ссылки. Надо перебирать все ссылки которые еще не просмотрены, удаляя ссылку перед загрузкой ее содержимого, и каждый раз когда найден новый URL, он должен быть добавлен в список необработанных ссылок. Когда список необработанных ссылок пуст, работа закончена – вы нашли все ссылки.</p>
<p>Хотя можно подумать, что: "Открытие сокета URL это операция связанная с URL, и следовательно ее тоже нужно сделать в классе  хранящем пару значений URL-глубина просмотра," это действие все таки выходит за рамки функций этого класса. Это только место для хранения значений  URL и глубины, с некоторыми дополнительными функциями их обработки. Сrawler, это класс который перемещается по веб страницам и ищет  в них ссылки, поэтому класс Сrawler  должен содержать код открывающий и закрывающий сокеты.</p>
<p>Вам нужно создать новый экземпляр класса <tt>Socket</tt> для каждого URL извлеченного из загруженного документа. Не забудьте закрыть сокет после того как вы закончите обработку страницы, иначе у операционной системы закончатся свободные номера портов! (Компьютер одновременно может держать открытыми ограниченное количество сокетов.) Также, не используйте рекурсию для поиска страниц на большей глубине; сделайте это с помощью цикла. Учитывая объем используемых ресурсов этот способ лучше.</p>
<p><b>Константы</b></p>
<p>В вашей программе несомненно будут использоваться строки вида <tt>"http://"</tt> и <tt>"a href=\""</tt>, и возможно вы собираетесь просто повторять их везде где требуется. Кроме этого в коде понадобятся длины этих строк используемые в различных строковых операциях, и возможно вы также собираетесь прямо указывать их в кодеso. <em><b>Не делайте так!!!</b></em> Это очень сильно затрудняет дальнейшую работу с кодом! Если вы сделаете опечатку, или в дальнейшем измените процедуру поиска, придется исправлять много разных строк кода.</p>
<p>Вместо этого, создайте в вашем классе строковые константы. Например, такую:</p>
<pre>    public static final String URL_PREFIX = "http://";</pre>
<p>Теперь если нужна строка, вместо того чтобы прямо вставлять ее, используйте константу <tt>URL_PREFIX</tt>. Если вам нужна длина этой строки, вам повезло - <tt>URL</tt><tt>_</tt><tt>PREFIX</tt> это объект типа <tt>String</tt>, а значит, вы можете вызвать метод <tt>URL_PREFIX.length()</tt> и получить длину строки.</p>
<p>Подумайте о том, куда поместить эти константы. Каждая константа должна быть объявлена в коде проекта только один раз, и она должна располагаться в наиболее подходящем месте. Например, так как префикс URL нужен для того чтобы определить имеет ли URL правильный формат, поместите его в класс для хранения пары значений  URL-глубина. Если у вас есть еще одна константа для HTML ссылок, поместите ее в ваш класс Crawler. Если классу Crawler понадобится, зачем либо, префикс URL, он может использовать константу из класса пары значений URL-глубина, вместо того чтобы дублировать эту константу у себя.</p>
<p><b>Дополнительные баллы</b></p>
<p>Напишите код, который  добавляет к списку ссылок для обработки только те ссылки, которые не обрабатывались раньше. Улучшите возможности робота по распознаванию ссылок в тексте, используя <a href="http://java.sun.com/j2se/1.4/docs/api/java/util/regex/package-summary.html">регулярные выражения</a> для поиска в собранных данных. Следует также добавить дополнительную логику для определения к какой машине следует подключиться следующий раз. Поисковый робот должен переходить по ссылкам<a href="http://www.google.com/">различных</a> <a href="http://www.slashdot.org/">популярных</a> <a href="http://www.caltech.edu/">сайтов</a>. Использование регулярных выражений требует дополнительного изучения, но в действительности реализуется гораздо проще, чем  поиск подстрок в строках (и это гораздо <b>более </b>мощное средство).</p>
<div align="center"><hr size="2" width="100%" noshade="noshade" align="center" /></div>
<p>Copyright (C) 2015, California Institute of Technology. All rights reserved.</p>
</body>
</html>